What is an Algorithm?
	An algorithm is the step-by-step unambigous instructions to solve a given problem.
	In the traditional study of algorithms, there are two main criteria for judging the merits of algorithms: 
	correctness (does the algorithm give solution to the problem in a finite number of steps?) and 
	efficiency (how much resources (in terms of memory and time) does it take to execute the).
	Note: We do not have to prove each step of the algorithm.
	Properties of Algorithm:
	1)0 or more inputs
	2)1 or more outputs
	3)Finiteness (Finite steps)
	4)Definiteness or Precision (Unambigous steps)
	5)Effectiveness(doable and effective steps)
	6)Correctness
	
Why the Analysis of Algorithms?
	Algorithm analysis helps us to determine which algorithm is most efficient in terms of time and space consumed.	
	The goal of the analysis of algorithms is to compare algorithms (or solutions) mainly in terms of running time 
	but also in terms of other factors (e.g., memory, developer effort, etc.)

What is Running Time Analysis?
	It is the process of determining how processing time increases as the size of the problem (input size) increases. 
	Input size is the number of elements in the input, and depending on the problem type, the input may be of different types. 
	The following are the common types of inputs.
	• Size of an array
	• Polynomial degree
	• Number of elements in a matrix
	• Number of bits in the binary representation of the input
	• Vertices and edges in a graph.

Why we need to learn it?
To measure the efficiency of algorithm.

How to Compare Algorithms
	To compare algorithms, let us define a few objective measures:
	Execution times? Not a good measure as execution times are specific to a particular computer.
	Number of statements executed? Not a good measure, since the number of statements varies 
		with the programming language as well as the style of the individual programmer.
	Ideal solution? Let us assume that we express the running time of a given algorithm as a function of the input size n (i.e., f(n)) 
	and compare these different functions corresponding to running times. 
	This kind of comparison is independent of machine time, programming style, etc.	

What is Rate of Growth?
	The rate at which the running time increases as a function of input is called rate of growth. 


Types of Analysis
• Worst case
	○ Defines the input for which the algorithm takes a long time (slowest time to complete).
	○ Input is the one for which the algorithm runs the slowest.
• Best case
	○ Defines the input for which the algorithm takes the least time (fastest time to complete).
	○ Input is the one for which the algorithm runs the fastest.
• Average case
	○ Provides a prediction about the running time of the algorithm.
	○ Run the algorithm many times, using many different inputs that come from some distribution that generates these inputs, 
		compute the total running time (by adding the individual times), and divide by the number of trials.
	○ Assumes that the input is random.
	
Lower Bound<=Average Time<=Upper Bound	

For a given algorithm, we can represent the best, worst and average cases in the form of expressions. 
As an example, let f(n) be the function, which represents the given algorithm.
	f(n)=n2+500, for worst case
	f(n)=n2+200n+500, for best case
Similarly for the average case. The expression defines the inputs with which the algorithm takes the average running time (or memory).

Notations
Analogy: Car mileage
Big O: Upper bound, like software for air bag in car
Omega O: Lower bound, for allocating resources
Theta:On an average
	Whether upper & lower bound is same or not

Example:
	Linear search: 
		Big-o: O(n)
		Omega-o: O(1)
		Theta-o: O(n\2)


If any algorithm does not have iteration or recursion then,
there is no dependency between running time and input size, it will take constant time as O(1).


Problem>Algorithm>Program

Commonly used Rates of Growth
	Time Complexity		Name
	O(1)				Constant			Addding an element at front of linked list
	O(log n)			Logarithmic			Finding an element in sorted array
	O(n)				Linear				Finding an element in unsorted array
	O(nlog n)			Linear Logarithmic	Merge Sort
	O(n^2)				Quardatic			Shortest path b/w 2 nodes in a graph
	O(n^3)				Cubic				Matrix Multiplication
	O(2^n)				Exponential			Tower of Hanoi


	((2)^2)^n
	n!
	4^n
	2^n
	n^2
	nlogn
	nog(n!)
	n
	2logn
	log2^n
	(logn)^(1/2)
	loglogn
	1
	
	
Asymptotic Notations
Big-O Notation
	This notation gives the tight upper bound of the given function. 
	Generally, it is represented as f(n)= O(g(n)). 
	That means, at larger values of n, the upper bound of f(n) is g(n).
	
	For example, if f(n) = n4 + 100n2 + 10n + 50 is the given algorithm, then n4 is g(n). 
	That means g(n) gives the maximum rate of growth for f(n) at larger values of n.
	
	Let us see the O–notation with a little more detail. O–notation defined as O(g(n)) = {f(n): 
	there exist positive constants c and n0 such that 0 ≤ f(n) ≤ cg(n) for all n ≥ n0}. 
	g(n) is an asymptotic tight upper bound for f(n). 
	Our objective is to give the smallest rate of growth g(n) which is greater than or equal to the given algorithms’ rate of growth f(n).

	Generally we discard lower values of n. That means the rate of growth at lower values of n is not important. 
	Here n0 is the point from which we need to consider the rate of growth for a given algorithm. 
	Below n0, the rate of growth could be different. n0 is called threshold for the given function.
	
	Big-O Visualization O(g(n)) is the set of functions with smaller or the same order of growth as g(n). For example;
	O(n2) includes O(1), O(n), O(nlogn), etc.
	Note: Analyze the algorithms at larger values of n only. 
	What this means is, below n0 we do not care about the rate of growth.
	
	Big-O Examples
	Example- Find upper bound for f(n) = 3n + 8
	Solution: 3n + 8 ≤ 4n, for all n ≥ 8
			∴ 3n + 8 = O(n) with c = 4 and n0 = 8
	Example- Find upper bound for f(n) = 410
	Solution: 410 ≤ 410, for all n ≥ 1
			∴ 410 = O(1) with c = 1 and n0 = 1
	
	No Uniqueness?
	There is no unique set of values for n0 and c in proving the asymptotic bounds. 
	Let us consider, 100n + 5 = O(n). For this function there are multiple n0 and c values possible.
	Solution1: 100n + 5 ≤ 100n + n = 101n ≤ 101n, for all n ≥ 5, n0 = 5 and c = 101 is a solution.
	Solution2: 100n + 5 ≤ 100n + 5n = 105n ≤ 105n, for all n ≥ 1, n0 = 1 and c = 105 is also a solution.
	
	If we use Θ notation to represent time complexity of Insertion sort, 
	we have to use two statements for best and worst cases:
		The worst case time complexity of Insertion Sort is Θ(n^2).
		The best case time complexity of Insertion Sort is Θ(n).
	
Omega-Ω Notation
	Similar to the O discussion, this notation gives the tighter lower bound of the given algorithm and we represent it as f(n) = Ω(g(n)). 
	That means, at larger values of n, the tighter lower bound of f(n) is g(n). 
	For example, if f(n) = 100n2 + 10n + 50, g(n) is Ω(n2).
	The Ω notation can be defined as Ω(g(n)) = {f(n): there exist positive constants c and n0 such that 0 ≤ cg(n) ≤ f(n) for all n ≥ n0}. 
	g(n) is an asymptotic tight lower bound for f(n). 
	Our objective is to give the largest rate of growth g(n) which is less than or equal to the given algorithm’s rate of growth f(n).
	
	Ω Examples
	Example-1 Find lower bound for f(n) = 5n2.
	Solution: ∃c, n0 Such that: 0 ≤ cn2≤ 5n2 ⇒ cn2≤ 5n2 ⇒ c = 5 and n0 = 1
	∴ 5n2 = Ω(n2) with c = 5 and n0 = 1
	Example-2 Prove f(n) = 100n + 5 ≠ Ω(n2).
	Solution: ∃ c, n0 Such that: 0 ≤ cn2 ≤ 100n + 5
				100n + 5 ≤ 100n + 5n (∀n ≥ 1) = 105n
				cn2 ≤ 105n ⇒ n(cn – 105) ≤ 0
				Since n is positive ⇒ cn – 105 ≤ 0 ⇒ n ≤ 105/c
				⇒ Contradiction: n cannot be smaller than a constant
	Example-3 2n = Ω(n), n3 = Ω(n3), logn = Ω(logn).
	
Theta-Θ Notation
The average running time of an algorithm is always between the lower bound and the upper bound. 

This notation decides whether the upper and lower bounds of a given function (algorithm) are the same. 
If the upper bound (O) and lower bound (Ω) give the same result, then the Θ notation will also have the same rate of growth. 
	As an example, let us assume that f(n) = 10n + n is the expression. 
	Then, its tight upper bound g(n) is O(n). The rate of growth in the best case is g(n) =O(n).
	In this case, the rates of growth in the best case and worst case are the same. As a result, the average case will also be the same.
	 
For a given function (algorithm), if the rates of growth (bounds) for O and Ω are not the same, then the rate of growth for the Θ case may not be the same.
In this case, we need to consider all possible time complexities and take the average of those (for example, for a quick sort average case, refer to the Sorting chapter).

Now consider the definition of Θ notation. 
It is defined as Θ(g(n)) = {f(n): there exist positive constants c1, c2 and n0 such that 0 ≤ c1g(n) ≤ f(n) ≤ c2g(n) for all n ≥ n0}. 
g(n) is an asymptotic tight bound for f(n). 
Θ(g(n)) is the set of functions with the same order of growth as g(n)

Θ Examples:
Example 1 Find Θ bound for f(n)=n2/2- n/2
Solution: n2/5<=n2/2-n/2<=n2, for all, n ≥ 2
∴n2/2- n/2=Θ(n2) with c1 = 1/5, c2 = 1 and n0 = 2
Example 2 Prove n ≠ Θ(n2)
Solution: c1n2 ≤ n ≤ c2n2 ⇒ only holds for: n ≤ 1/c1
			∴ n ≠ Θ(n2)
Example 3 Prove 6n3 ≠ Θ(n2)
Solution: c1 n2≤ 6n3 ≤ c2n2 ⇒ only holds for: n ≤ c2/6
		∴ 6n3 ≠ Θ(n2)
Example 4 Prove n ≠ Θ (logn)

1.17 Important Notes
From the above examples, it should also be clear that, for a given function (algorithm), 
	getting the upper bound (O) and lower bound (Ω) and average running time (Θ) may not always be possible.
In the remaining chapters, we generally focus on the upper bound (O) 
	because knowing the lower bound (Ω) of an algorithm is of no practical importance, 
	and we use the Θ notation if the upper bound (O) and lower bound (Ω) are the same.

1.18 Why is it called Asymptotic Analysis?
From the discussion above (for all three notations: worst case, best case, and average case), we can easily understand that, 
	in every case for a given function f(n) we are trying to find another function g(n) which approximates f(n) at higher values of n. 
	That means g(n) is also a curve which approximates f(n) at higher values of n.
	In mathematics we call such a curve an asymptotic curve. 
	In other terms, g(n) is the asymptotic curve for f(n). 
	For this reason, we call algorithm analysis asymptotic analysis.
	
1.19 Guidelines for Asymptotic Analysis
There are some general rules to help us determine the running time of an algorithm.
1) Loops: The running time of a loop is, at most, the running time of the statements inside the loop (including tests) multiplied by the number of iterations.
	Total time = a constant c × n = c n = O(n).
2) Nested loops: Analyze from the inside out. Total running time is the product of the sizes of all the loops.
	Total time =c × n × n = cn2 = O(n2).
3) Consecutive statements: Add the time complexities of each statement.
	Total time = c0 + c1n + c2n2 = O(n2)
4) If-then-else statements: Worst-case running time: the test, plus either the then part or the else part (whichever is the larger).
5) Logarithmic complexity: An algorithm is O(logn) if it takes a constant time to cut the problem size by a fraction (usually by½). 
	As an example let us consider the following program:
	for(int i=1;i<=n;)
	{
		i=i*2;
	}	
	Let us assume that the loop is executing some k times. At kth step 2k = n, and at (k + 1)th step we come out of the loop. 
	Taking logarithm on both sides, gives
	 log(2k)=logn 
	 klog2=logn
	 k=logn  //If we assume base2
	 Total time = O(logn).
	 Note: Similarly, for the case below, the worst case rate of growth is O(logn). The same discussion holds good for the decreasing sequence as well.
	 for(int i=n;i>=1;)
	  {
		i=i/2;
	 }
	 Another example: binary search (finding a word in a dictionary of n pages)
		• Look at the center point in the dictionary
		• Is the word towards the left or right of center?
		• Repeat the process with the left or right part of the dictionary until the word is found.
		
1.20 Simplyfying properties of asymptotic notations
• Transitivity: f(n) = Θ(g(n)) and g(n) = Θ(h(n)) ⇒f(n) = Θ(h(n)). Valid for O and Ω as well.
• Reflexivity: f(n) = Θ(f(n)). Valid for O and Ω.
• Symmetry: f(n) = Θ(g(n)) if and only if g(n) = Θ(f(n)).
• Transpose symmetry: f(n) = O(g(n)) if and only if g(n) = Ω(f(n)).
• If f(n) is in O(kg(n)) for any constant k > 0, then f(n) is in O(g(n)).
• If f1(n) is in O((g1(n)) and f2(n) is in O(g2(n)), then (f1 + f2)(n) is in O(max(g1(n),g1(n))).
• If f1(n) is in O(g1(n)) and f2(n) is in O(g2(n)) then f1(n) f2(n) is in O(g1(n) g1(n)).

1.21 Commonly used Logarithms and Summations
	Logarithms
	Arithmetic series
	Geometric series
	Harmonic series
	Other important formulae

1.22 Master Theorem for Divide and Conquer Recurrences
All divide and conquer algorithms  divide the problem into sub-problems, each of which is part of the original problem, 
	and then perform some additional work to compute the final answer. 
	As an example, a merge sort algorithm operates on two sub-problems, each of which is half the size of the original, 
		and then performs O(n) additional work for merging. 
	This gives the running time equation:
		T(n)=2T(n/2)+O(n)
	The following theorem can be used to determine the running time of divide and conquer algorithms. 
	For a given program (algorithm), first we try to find the recurrence relation for the problem. 
	If the recurrence is of the below form then we can directly give the answer without fully solving it. 
	If the recurrence is of the form T(n)=aT(n/b)+Θ(nk(logn)p), where a ≥ 1, b > l,k≥ 0 and p is a real number, then:
	1) If a > bk, then T(n)=Θ(n^(loga/logb))
	2) If a = bk
		a. If p > –1, then T(n)=Θ((n^(loga/logb))* (logn)^(p+1))
		b. If p = –1, then T(n)=Θ((n^(loga/logb))* loglogn)
		c. If p < –1, then T(n)=Θ(n^(loga/logb))
	3) If a < bk
		a. If p ≥ 0, then T(n)=Θ(n^k*(logn)^p)
		b. If p < 0, then T(n) = O(nk)
		
1.23 Divide and Conquer Master Theorem: Problems & Solutions

1.24 Master Theorem for Subtract and Conquer Recurrences

1.25 Variant of Subtraction and Conquer Master Theorem
The solution to the equation T(n) = T(α n) + T((1 – α)n) + βn, where 0 < α < 1 and β > 0 are constants, is O(nlogn).

1.26 Method of Guessing and Confirming
Now, let us discuss a method which can be used to solve any recurrence. 
The basic idea behind this method is:
 guess the answer and then prove it correct by induction
In other words, it addresses the question: 
	What if the given recurrence doesn’t seem to match with any of these (master theorem) methods? 
	If we guess a solution and then try to verify our guess inductively, 
		usually either the proof will succeed (in which case we are done), 
		or the proof will fail (in which case the failure will help us refine our guess).
 	
1.27 Amortized Analysis
Amortized analysis refers to determining the time-averaged running time for a sequence of operations. 
	It is different from average case analysis, because amortized analysis does not make any assumption about the distribution of the data values, 
	whereas average case analysis assumes the data are not “bad” (e.g., some sorting algorithms do well on average over all input orderings 
	but very badly on certain input orderings). 
	That is, amortized analysis is a worst-case analysis, but for a sequence of operations rather than for individual operations.
The motivation for amortized analysis is to better understand the running time of certain techniques, 
	where standard worst case analysis provides an overly pessimistic bound. 
	Amortized analysis generally applies to a method that consists of a sequence of operations, 
	where the vast majority of the operations are cheap, but some of the operations are expensive. 
	If we can show that the expensive operations are particularly rare we can change them to the cheap operations,
	and only bound the cheap operations.
The general approach is to assign an artificial cost to each operation in the sequence, 
	such that the total of the artificial costs for the sequence of operations bounds the total of the real costs for the sequence. 
	This artificial cost is called the amortized cost of an operation. 
	To analyze the running time, the amortized cost thus is a correct way of understanding the overall running time – 
	but note that particular operations can still take longer so it is not a way of bounding the running time of any individual operation in the sequence.
When one event in a sequence affects the cost of later events:
	• One particular task may be expensive.
	• But it may leave data structure in a state that the next few operations becomes easier.
Example: Let us consider an array of elements from which we want to find the kth smallest element. 
We can solve this problem using sorting. After sorting the given array, we just need to return the kth element from it. 
The cost of performing the sort (assuming comparison based sorting algorithm) is O(nlogn). 
If we perform n such selections then the average cost of each selection is O(nlogn/n) = O(logn). 
This clearly indicates that sorting once is reducing the complexity of subsequent operations.

1.28 Algorithms Analysis: Problems & Solutions
Note: From the following problems, try to understand the cases which have different complexities (O(n),O(logn),O(loglogn) etc.).



Example:
sum(n): Algorithm to add first n natural numbers. Assume s is a variable initialized to 0.
1. if n<=0 return -1
2. repeat step 3 and 4 while n!=0
3. s=s+n
4. [decrement n]n--
5. return s

sum(n): Algorithm to add first n natural numbers. Assume s is a variable initialized to 0 & i is a variable initialized to 1.
1. if n<=0 return -1
2. repeat step 3 and 4 while i<=n
3. s=s+i
4. [increment i]i++
5. return s








Analysis of Loops
1) O(1): 
Time complexity of a function (or set of statements) is considered as O(1) 
	if it doesn't contain loop, recursion and call to any other non-constant time function.
   // set of non-recursive and non-loop statements
For example swap() function has O(1) time complexity.
A loop or recursion that runs a constant number of times is also considered as O(1). 
For example the following loop is O(1).
   // Here c is a constant   
   for (int i = 1; i <= c; i++) {  
        // some O(1) expressions
   }

2)O(n): 
Time Complexity of a loop is considered as O(n) 
if the loop variables is incremented / decremented by a constant amount. 
For example following functions have O(n) time complexity.
   // Here c is a positive integer constant   
   for (int i = 1; i <= n; i += c) {  
        // some O(1) expressions
   }
   for (int i = n; i > 0; i -= c) {
        // some O(1) expressions
   }
   
3)O(n^c): 
Time complexity of nested loops is equal to the number of times the innermost statement is executed. 
For example the following sample loops have O(n2) time complexity
   for (int i = 1; i <=n; i += c) {
       for (int j = 1; j <=n; j += c) {
          // some O(1) expressions
       }
   }
   for (int i = n; i > 0; i -= c) {
       for (int j = i+1; j <=n; j += c) {
          // some O(1) expressions
   }
  
4)O(Logn): 
Time Complexity of a loop is considered as O(Logn) 
if the loop variables is divided / multiplied by a constant amount.
   for (int i = 1; i <=n; i *= c) {
       // some O(1) expressions
   }
   for (int i = n; i > 0; i /= c) {
       // some O(1) expressions
   }
For example Binary Search(refer iterative implementation) has O(Logn) time complexity. 
Let us see mathematically how it is O(Log n). 
The series that we get in first loop is 1, c, c2, c3, ... ck. 
If we put k equals to Logcn, we get cLogcn which is n.

5)O(LogLogn): 
Time Complexity of a loop is considered as O(LogLogn) 
if the loop variables is reduced / increased exponentially by a constant amount.
   // Here c is a constant greater than 1   
   for (int i = 2; i <=n; i = pow(i, c)) { 
       // some O(1) expressions
   }
   // Here fun() is function to find square root  
   // or cuberoot or any other constant root
   for (int i = n; i > 1; i = fun(i)) { 
       // some O(1) expressions
   }
   
   
 How to combine time complexities of consecutive loops? 
 When there are consecutive loops, we calculate time complexity as sum of time complexities of individual loops.
   for (int i = 1; i <=m; i += c) {  
        // some O(1) expressions
   }
   for (int i = 1; i <=n; i += c) {
        // some O(1) expressions
   }
   Time complexity of above code is O(m) + O(n) which is O(m+n)
   If m == n, the time complexity becomes O(2n) which is O(n).
   
      
How to calculate time complexity when there are many if, else statements inside loops? 
As discussed earlier, the worst-case time complexity is the most useful among best, average and worst. 
Therefore we need to consider the worst case. 
We evaluate the situation when values in if-else conditions cause a maximum number of statements to be executed.

For example, consider the linear search function where we considered the case when an element is present at the end or not present at all.
When the code is too complex to consider all if-else cases, we can get an upper bound by ignoring if else and other complex control statements.



Analysis of recursion
1)Using Master Theorem
2)Recursion tree method


Time complexity
Amount of time taken by an algorithm to run as a function of the length of input.



Running time depends upon:
input 
Single or multiprocessor machine
Read/write speed of memory
32 bit/64 bit achitecture


Time Space complexity trade-off
	

Finding Runtime Complexities of Iterative Algo
	O(n)=O(n-1)=O(2*n)
	O(1)=O(1000)
Finding Runtime Complexities of Recursive Algo
	Udemy:S02,L06,L07:DSA
	Back Substitution
	Master Theorem

